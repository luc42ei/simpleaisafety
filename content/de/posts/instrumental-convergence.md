---
title: "Instrumentelle Konvergenz"
date: 2023-09-01T17:45:20+02:00
summary: Intelligente Individuen haben einen Anreiz instrumentelle Ziele zu verfolgen, die für eine breite Palette an Endzielen nützlich sind.
---

![Instrumentelle Konvergenz](/instrumental_convergence.png 'Es gibt einige instrumentelle Teilziele (dargestellt durch das Warnzeichen), die viele intelligente Agenten (die Kreuze) verfolgen werden, meist unabhängig von ihren Endzielen.')

# Grundlagen

Instrumentelle Konvergenz bedeutet, dass Menschen oder andere Wesen mit unterschiedlichen Endzielen oft ähnliche Zwischenstrategien, so genannte instrumentelle Ziele, anwenden, um ihre eigentlichen Ziele zu erreichen. Denkt man bspw. an drei Freunde, die sich zum Wissenschaftler, zum Anwalt und zum Sportler ausbilden lassen, werden alle von Zeitmanagementfähigkeiten profitieren, um ihre Verantwortlichkeiten im Griff zu behalten, sowie von sozialen Fähigkeiten, um nützliche Netzwerke aufzubauen, und von einer gesunden Menge an Schlaf, um sich von ihren vollen Terminkalendern zu erholen. Im Zusammenhang mit KI bedeutet dies, dass manche KI-Systeme motiviert sind, bestimmte instrumentelle Ziele zu erreichen. Typische Beispiele dafür sind:

* der Erwerb von Ressourcen
* die Verbesserung ihrer Fähigkeiten
* die Selbsterhaltung, um zu verhindern, dass sie abgeschaltet werden

Wenn beispielsweise ein hochintelligenter Roboter darauf programmiert wäre, Kaffee zu holen, würde er sich gegen Versuche wehren, abgeschaltet zu werden, auch wenn er nicht explizit auf Selbsterhaltung ausgelegt ist, einfach weil "man keinen Kaffee holen kann, wenn man tot ist", wie Stuart Russell es ausdrückt. Die Selbsterhaltung ist ein instrumentelles Ziel, das sicherstellt, dass das Endziel erreicht werden kann.

# Weiterführend

The concept of instrumental convergence is important to understand when considering the likely goals of advanced AI systems. It describes how various intelligent agents with different ultimate objectives are likely to converge on similar intermediate or "instrumental" goals that are essential for achieving their respective end goals. An example of this is two different ML systems: one designed to develop personalized medicine and another for algorithmic trading. While their ultimate goals are vastly different, they might both benefit from instrumental goals like minimizing latency, maximizing data throughput, or avoiding overfitting. These are convergent instrumental objectives because they are generally useful across different optimization problems.

More general and advanced ML systems could potentially share instrumental goals which pose significant risks to humanity, such as resource acquisition, self-preservation, and elimination of threats. It isn’t hard to imagine that a system which pursues these goals would constitute a threat to humanity, as it would actively avoid being shut down, and seek to prevent other agents from meddling in its interests.

# Fortgeschritten

- https://arbital.com/p/instrumental_convergence/
- https://www.lesswrong.com/tag/instrumental-convergence 
- https://philosophicaldisquisitions.blogspot.com/2014/07/bostrom-on-superintelligence-2.html 
- Why Would AI Want To Do Bad Things https://www.youtube.com/watch?v=ZeecOKBus3Q 
- Optimal Policies Tend To Seek Power https://arxiv.org/pdf/1912.01683.pdf 
